---
title: "Assignment: VAST Challenge 2021 (Mini-Challenge 2)"
description: |
  Investigating the Mini-Challenge 2 of VAST Challenge 2021 as part of the Assignment
author:
  - name: Syed Ahmad Zaki
    url: https://www.google.com/
date: 07-07-2021
output:
  distill::distill_article:
    self_contained: false
    code_folding: true
    toc: true
    toc_depth: 4
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Installing and loading necessary packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(ggplot2, 
               tidyverse, 
               readr, 
#               jpeg, 
               grid, 
               plotly, 
#               gganimate, 
               writexl, 
               qwraps2, 
               reshape2, 
               scales, 
               sf, 
               raster, 
               tmap, 
               clock,
               forcats,
               readxl,
               lubridate)

```

**Team Member:**  
Syed Ahmad Zaki, Singapore Management University of Singapore, ahmadzaki.2020@mitb.smu.edu.sg   
Student Team:  YES  

**Tools Used:**  
Rmarkdown  

**Approximately how many hours were spent working on this submission in total?**  
Provide an estimate of the total number of hours worked on this submission by your entire team.  

**May we post your submission in the Visual Analytics Benchmark Repository after VAST Challenge 2021 is complete?**  
YES  

**Video**  
Provide a link to your video.  Example:
http://www.westbirmingham.ac.uk/uwb-smith-mc2-video.wmv

--- 

## 1. Introduction
### 1.1 Our Mission (Should We Accept It!)
As a visual analytics expert assisting law enforcement, your mission is to identify which GASTech employees made which purchases and identify suspicious patterns of behavior. You must cope with uncertainties that result from missing, conflicting, and imperfect data to make recommendations for further investigation.

Use visual analytics to analyze the available data and develop responses to the questions below. In addition, prepare a video that shows how you used visual analytics to solve this challenge. Submission instructions are available here. Entry forms are available for download [here.](https://vast-challenge.github.io/2021/MC2.html)

---

## 2.	Literature Review
A cursory look at the dataset reveals the following data types:

| Data                | Type        | Description                                         |
| :---:               |    :----:   |         :---                                        |
| Credit Card.csv     | Aspatial    | Credit card txns by timestamp, location and amt     |
| Loyalty.csv         | Aspatial    | Loyalty card txns by date, location and amt         |    
| Car Assignment.csv  | Aspatial    | Car assignment ID with individuals' name and role   |
| MC2.jpg             | Aspatial    | Abila's map in jpeg format                          |
| MC2.tif             | Geospatial  | Abila's map in a geotiff format                     |
| GPS.csv             | Geospatial  | GPS points (latlong) by car ID and timestamp        |
| Abila               | Geospatial  | Abila's road network                                |
| Kronos Island       | Geospatial  | Polygon showing Kronos Island's admin boundary      |

With these dataset in mind, the following considerations would need to be addressed:

### 2.1 Fuzzy Matching (Credit Card with Loyalty Data)

Explain levenshtein distance here


### 2.2 Visualising Large Counts of GPS Data

ABCDE


---

## 3.	Data Understanding
We start by loading all the necessary datasets provided in the VAST Challenge 2021 Mini-Challenge 2.

```{r load-all-datasets, echo = T, results = 'hide'}

# Loading all datasets and image
cc <- readr::read_csv("data/cc_data.csv") # Add credit card data
loyalty <- readr::read_csv("data/loyalty_data.csv") # Add loyalty data
# mc2 <- jpeg::readJPEG("img/MC2-tourist.jpg", native = TRUE) # Add jpeg map file
mc2 <- raster("data/MC2-tourist_modified.tif") # Add tif file as a raster layer
gps <- readr::read_csv("data/gps.csv") # Add gps data
car <- readr::read_csv("data/car-assignments.csv") # Add car assignments
Abila_st <- st_read(dsn = "data", layer = "Abila")

```

### 3.1	Understanding the data
As always, we review each dataset in greater detail. This is a necessary step in order to accurately prepare the data for subsequent use.

We will begin by reviewing all four csv data first. We immediately noticed a few discrepancies:  
1. Date format within the timestamp were in a MM-DD-YYYY H:M format  
2. Katerina's Cafe contains unique characters, which may cause downstream problems during our analysis  
3. ID and Last4CCNum are treated as regular double numbers, instead of a character type.

```{r csv-data-cleaning}

#--------------- Cleaning CC data ---------------

#cc$timestamp <- as.POSIXct(cc$timestamp, format = "%m/%d/%Y  %H:%M", tz = "GMT") #Readjust CC timestamp
#cc$timestamp <- as.character(cc$timestamp)
cc$timestamp <- date_time_parse(cc$timestamp,
                zone = "",
                format = "%m/%d/%Y  %H:%M") #Readjust CC timestamp
cc[grep("Katerina", cc$location),2] <- "Katerina's Cafe" #Replace unique characters in Katerina's Cafe
cc$last4ccnum <- as_factor(cc$last4ccnum)
cc$hour <- hour(cc$timestamp)
cc$period <- case_when(
  cc$hour >= 21 ~ "Late Evening 9pm to 11.59pm",
  cc$hour >= 18 ~ "Evening 6pm to 8.59pm",
  cc$hour >= 12 ~ "Afternoon 12noon to 5.59pm",
  cc$hour >= 6 ~ "Morning 6am to 11.59am",
  TRUE ~ "Late Night 12mn to 5.59am"
)
cc$dayofmonth <- day(cc$timestamp)
cc$weekday <- wday(cc$timestamp, label = TRUE)

#--------------- Cleaning Loyalty data ---------------

#loyalty$timestamp <- as.POSIXct(loyalty$timestamp, format = "%m/%d/%Y", tz = "GMT") #Readjust Loyalty timestamp
loyalty$timestamp <- date_time_parse(loyalty$timestamp,
                zone = "",
                format = "%m/%d/%Y") #Readjust CC timestamp
loyalty[grep("Katerina", loyalty$location),2] <- "Katerina's Cafe" #Replace unique characters in Katerina's Cafe
loyalty$dayofmonth <- day(loyalty$timestamp)
loyalty$weekday <- wday(loyalty$timestamp, label = TRUE)

#--------------- Cleaning Car Assignment data ---------------

car$CarID <- as_factor(car$CarID)
car$FullName <- paste(car$FirstName,car$LastName, sep = " ")

#--------------- Cleaning GPS data ---------------

gps$id <- as_factor(gps$id)
gps$Timestamp <- date_time_parse(gps$Timestamp,
                zone = "",
                format = "%m/%d/%Y %H:%M:%S")
gps$date <- as_date(gps$Timestamp)
gps$hour <- hour(gps$Timestamp)
gps$period <- case_when(
  gps$hour >= 21 ~ "Late Evening 9pm to 11.59pm",
  gps$hour >= 18 ~ "Evening 6pm to 8.59pm",
  gps$hour >= 12 ~ "Afternoon 12noon to 5.59pm",
  gps$hour >= 6 ~ "Morning 6am to 11.59am",
  TRUE ~ "Late Night 12mn to 5.59am"
)
gps$dayofmonth <- day(gps$Timestamp)
gps$weekday <- wday(gps$Timestamp, label = TRUE)

```

---

## 4.	Data Preparation
### 4.1	Combining Both Credit Card and Loyalty Data
We will now attempt to find matching rows between the cc and loyalty data.  

#### 4.1.1 Fuzzy String Matching Using Levenshtein Distance  
We will use [fuzzy string matching](https://www.r-bloggers.com/2015/02/fuzzy-string-matching-a-survival-skill-to-tackle-unstructured-information/) using [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) which is available natively in R's adist utilities package.

```{r levenshtein-distance-matching-cc-loyalty}

cc <- tibble::rowid_to_column(cc, "ID") #Create a numeric id column
loyalty <- tibble::rowid_to_column(loyalty, "ID") #Create a numeric id column
cc$date <- as.Date(cc$timestamp) #Create a separate column just for dates in the cc data
cc$hour <- as.numeric(format(cc$timestamp,"%H")) #Create a separate column just for hours in the cc data
cc$concat <- paste(cc$date,cc$location,cc$price) #Create a separate column of unique values using concatenated values in the cc data
loyalty$concat <- paste(loyalty$timestamp,loyalty$location,loyalty$price) #Create a separate column of unique values using concatenated values in the loyalty data
dist.concat <- adist(cc$concat,loyalty$concat, partial = TRUE, ignore.case = TRUE) #Creates a matrix with the Standard Levenshtein distance between both newly created concat columns
min.concat <- apply(dist.concat, 1, min) #Extract pairs with minimum distance

match.s1.s2 <- NULL
for (i in 1:nrow(dist.concat))
{
  s2.i <- match(min.concat[i], dist.concat[i,])
  s1.i <- i
  match.s1.s2 <- rbind(data.frame(loyalty.i=s2.i,
                                    cc.i=s1.i,
                                    loyalty_concat=loyalty[s2.i,]$concat,
                                    cc_concat=cc[s1.i,]$concat,
                                    adist=min.concat[i]),match.s1.s2)
  }
    
cc_loyalty <- match.s1.s2 %>%
  left_join(dplyr::select(cc, last4ccnum, ID), by = c("cc.i" = "ID")) %>% #Add in CC num column
  left_join(dplyr::select(loyalty, loyaltynum, ID), by = c("loyalty.i" = "ID")) #Add in loyalty card num column

```

#### 4.1.2 Extracting Best Matching Pairs     
Let's now extract the matching credit card-loyalty pairs according to 80% matching of their comparative distance.

```{r extract-cc-loyalty-pairs}

cc_loyalty_unique <- dcast(cc_loyalty, last4ccnum + loyaltynum ~ adist) #Long to wide by transposing adist
cc_loyalty_unique$Total <- rowSums(cc_loyalty_unique[,c("0","1","2","3","4","5","11")]) #Sum all rows
cc_loyalty_unique$Sum01 <- rowSums(cc_loyalty_unique[,c("0","1")]) #Sum only column 1 and 2
cc_loyalty_unique$MatchPctTotal <- percent(cc_loyalty_unique[,3] / cc_loyalty_unique$Total) #Calc % of perfect (0) matches against Total
cc_loyalty_unique$MatchPct01 <- percent(cc_loyalty_unique$Sum01 / cc_loyalty_unique$Total) #Calc % of perfect (0) and almost perfect (0) matches against Total

cc_loyalty_unique_80 <- cc_loyalty_unique %>% 
  filter (MatchPct01 >= "80.00%")
n_distinct(cc_loyalty_unique_80$last4ccnum)
n_distinct(cc_loyalty_unique_80$loyaltynum)

write_csv(cc_loyalty_unique_80,"C:\\Users\\syeda\\OneDrive\\Documents\\SMU Courses\\2021T3 ISSS608 Visual Analytics and Applications\\3. Submissions\\Assignments\\Sandbox\\cc_loyalty_unique_80.csv")

```








### 4.2	Combining Both GPS and Car Assignment Data
#### 4.2.1 Initial Step
First, we will merge the GPS data with the car assignments.

```{r GPS-data-manipulation}

# GPS Data Manipulation
gps_name <- left_join(gps,car, by = c("id" = "CarID")) # Merge car assignments to gps data
gps_name$RoleNName <- paste(gps_name$id, gps_name$CurrentEmploymentTitle, gps_name$FullName, sep = " ")
gps_name$Timestamp <- as.POSIXct(gps_name$Timestamp, format = "%m/%d/%Y  %H:%M:%S", tz = "GMT") # Timestamp switching to month-day-year format
gps_name <- gps_name[with(gps_name,order(id,Timestamp)),] # Sort first by ID in ascending order and then Timestamp by oldest to newest
gps_name <- gps_name %>% # Add running number in the first column
  mutate(No = 1:n()) %>% 
  dplyr::select(No, everything()) 
gps_name <- gps_name %>% # Create additional column indicating time taken from previous timestamp for same ID
    mutate(Delta = Timestamp - lag(Timestamp, default = first(Timestamp)))

gps_name$Delta <- as.numeric(gps_name$Delta)
gps_name$Delta_Hours <- round(gps_name$Delta / 60 / 60, 1) # Create column to convert Delta seconds into hours with one decimal place

```

#### 4.2.2 Isolate Stationary GPS Points
Next, we will isolate GPS points, that have been stationary for at least 10 mins.

```{r GPS-stationary}

spots <- gps_name %>% # Filtering out gps coordinates where stationary for more than 10 mins
  filter(Delta > 600)
spots$No <- rep(1:2965, times = 1) # Redo running number in the first column

```

#### 4.2.3 Identifying Stationary GPS Points
Next, using the map and other data sources, we identify the locations of each of these stationary GPS points.

```{r identifying-stationary-GPS-points}

spots$Location <- 1 # Create a Location column

spots <- spots %>% mutate(
  Location = case_when(
    between(lat, 36.05092013, 36.05102938) & 
      between(long, 24.82586806, 24.82598723)  ~ "Abila Airport", # 35 features
    between(lat, 36.07434876, 36.07443715) & 
      between(long, 24.84592966, 24.84598782)  ~ "Abila Scrapyard", # 4 features
    between(lat, 36.06342076, 36.06349309) & 
      between(long, 24.85096457, 24.85103679)  ~ "Abila Zacharo", # 66 features
    between(lat, 36.07660977, 36.07669909) & 
      between(long, 24.85756408, 24.85764247)  ~ "Albert's Fine Clothing", # 155 features
    between(lat, 36.08172086, 36.08182543) & 
      between(long, 24.85086882, 24.85096705)  ~ "Bean There Done That", # 46 features
    between(lat, 36.05572125, 36.05584094) & 
      between(long, 24.90246542, 24.90258487)  ~ "Brew've Been Served", # 143 features
    between(lat, 36.06582469, 36.065941) & 
      between(long, 24.90097567, 24.90108865)  ~ "Building Control Stenig's Home", # 20 features
    between(lat, 36.05851786, 36.05860144) & 
      between(long, 24.8808655, 24.88092654)  ~ "Carlyle Chemical Inc.", # 30 features
    between(lat, 36.05859158, 36.05859887) & 
      between(long, 24.85790261, 24.85799357)  ~ "Carly's Coffee", # 3 features
    between(lat, 36.07818062, 36.07821857) & 
      between(long, 24.87211555, 24.8721508)  ~ "CFO Ingrid's Home", # 27 features
    between(lat, 36.07682044, 36.07685752) & 
      between(long, 24.8658641, 24.86589901)  ~ "CIO Ada's Home", # 35 features
    between(lat, 36.0721156, 36.07215701) & 
      between(long, 24.87458425, 24.8746267)  ~ "COO Orhan's Home", # 29 features
    between(lat, 36.07062423, 36.07073983) & 
      between(long, 24.89517609, 24.89526281)  ~ "Chostus Hotel", # 11 features
    between(lat, 36.05462322, 36.05469486) & 
      between(long, 24.88977034, 24.88983886)  ~ "Coffee Cameleon", # 29 features
    between(lat, 36.07332048, 36.07336116) & 
      between(long, 24.86416419, 24.86420583)  ~ "Daily Dealz", # 36 features
    between(lat, 36.07292088, 36.07301365) & 
      between(long, 24.88396447, 24.88405897)  ~ "Drill Site Manager Marin's Home", # 26 features
    between(lat, 36.08442031, 36.08449538) & 
      between(long, 24.86416741, 24.8642387)  ~ "Drill Technician Elsa's Home", # 25 features
    between(lat, 36.08424703, 36.08432477) & 
      between(long, 24.8563809, 24.8564637)  ~ "Drill Technician Gustav's Home", # 13 features
    between(lat, 36.0726185, 36.07380904) & 
      between(long, 24.87510166, 24.87613744)  ~ "Drill Technician Isande's Home", # 26 features
    between(lat, 36.06922564, 36.06931513) & 
      between(long, 24.88416486, 24.88426267)  ~ "Drill Technician Kare's Home", # 20 features
    between(lat, 36.08542073, 36.08550845) & 
      between(long, 24.86036422, 24.86045943)  ~ "Engineer Lars's Home", # 37 features
    between(lat, 36.08664252, 36.08672442) & 
      between(long, 24.85756416, 24.85766744)  ~ "Engineer Felix's Home", # 22 features
    between(lat, 36.07622023, 36.07626546) & 
      between(long, 24.87466429, 24.87471053)  ~ "Environmental Safety Advisor Willem's Home", # 33 features
    between(lat, 36.07212045, 36.07213193) & 
      between(long, 24.84132949, 24.84134818)  ~ "Frank's Fuel", # 2 features
    between(lat, 36.0603222, 36.06044736) & 
      between(long, 24.90556693, 24.90569385)  ~ "Frydos Autosupply n' More", # 85 features
    between(lat, 36.04802098, 36.04805422) & 
      between(long, 24.87956497, 24.87957691)  ~ "GasTech & Jack's Magical Beans", # 738 features
    between(lat, 36.05970763, 36.05981097) & 
      between(long, 24.85797552, 24.8580772)  ~ "General Grocer", # 47 features
    between(lat, 36.06362146, 36.06371539) & 
      between(long, 24.88586605, 24.88595859)  ~ "Hallowed Grounds", # 70 features
     between(lat, 36.08412146, 36.08420924) & 
      between(long, 24.85896842, 24.85905081)  ~ "Hydraulic Technician Axel's Home", # 23 features
     between(lat, 36.08782802, 36.08793196) & 
      between(long, 24.85627136, 24.8563725)  ~ "Hydraulic Technician Vira's Home", # 24 features
    between(lat, 36.06641679, 36.06650723) & 
      between(long, 24.88256875, 24.88265687)  ~ "IT Helpdesk Nils's Home", # 31 features
    between(lat, 36.06729646, 36.06736745) & 
      between(long, 24.87788423, 24.87795559)  ~ "IT Technician Isak's Home", # 21 features
    between(lat, 36.06722012, 36.06731624) & 
      between(long, 24.8858687, 24.88596759)  ~ "IT Technician Lucas's Home", # 23 features
    between(lat, 36.06582037, 36.06584879) & 
      between(long, 24.85236427, 24.85241027)  ~ "Kalami Kafenion", # 47 features
    between(lat, 36.05442247, 36.05453641) & 
      between(long, 24.89986596, 24.89998054)  ~ "Katerina’s Café", # 158 features
    between(lat, 36.05840347, 36.05849041) & 
      between(long, 24.88546548, 24.88553455)  ~ "Nationwide Refinery", # 41 features
    between(lat, 36.05192066, 36.05197575) & 
      between(long, 24.87076418, 24.87082137)  ~ "Ouzeri Elian", # 67 features
    between(lat, 36.06764972, 36.06775002) & 
      between(long, 24.90243213, 24.9025445)  ~ "Perimeter Control Edvard's Home", # 20 features
    between(lat, 36.05942407, 36.05952152) & 
      between(long, 24.89476557, 24.8948649)  ~ "Shared Home A - Linnea Kanon Bertrand", # 72 features
    between(lat, 36.06332304, 36.06343537) & 
      between(long, 24.89607033, 24.89617856)  ~ "Shared Home B - Lidelse Birgitta Hennie", # 60 features
    between(lat, 36.06242283, 36.06253955) & 
      between(long, 24.89877023, 24.89888179)  ~ "Shared Home C - Sven Minke Brand", # 68 features
    between(lat, 36.05842222, 36.05853828) & 
      between(long, 24.90096522, 24.90107874)  ~ "Shared Home D - Adra Varja Felix", # 73 features
    between(lat, 36.06772112, 36.06784956) & 
      between(long, 24.89906521, 24.89917328)  ~ "Site Control Hideki's Home", # 21 features
    between(lat, 36.06774029, 36.06776587) & 
      between(long, 24.87148791, 24.87150031)  ~ "U-Pump", # 4 features      
    ))

sum(is.na(spots$Location))
length(grep("Daily Dealz", spots$Location))

write_csv(spots,"C:\\Users\\syeda\\OneDrive\\Documents\\SMU Courses\\2021T3 ISSS608 Visual Analytics and Applications\\3. Submissions\\Assignments\\Sandbox\\spots_latest.csv")

#write_csv(cc,"C:\\Users\\syeda\\OneDrive\\Documents\\SMU Courses\\2021T3 ISSS608 Visual Analytics and Applications\\3. Submissions\\Assignments\\Sandbox\\cc_date.csv")

```



```{r levenshtein-distance-matching-cc-GPS}

cc$concat2 <- paste(cc$date,cc$location,cc$hour) #Create a second separate column of unique values using concatenated values in the cc data
spots$concat <- paste(spots$date,spots$Location,spots$hour) #Create a separate column of unique values using concatenated values in the distilled GPS data
dist.concat2 <- adist(cc$concat2,spots$concat, partial = TRUE, ignore.case = TRUE) #Create a matrix with the Standard Levenshtein distance between both newly created concat columns
min.concat2 <- apply(dist.concat2, 1, min) #Extract pairs with minimum distance

match.s3.s4 <- NULL
for (j in 1:nrow(dist.concat2))
{
  s4.j <- match(min.concat2[j], dist.concat2[j,])
  s3.j <- j
  match.s3.s4 <- rbind(data.frame(spots.j=s4.j,
                                    cc.j=s3.j,
                                    spots_concat=spots[s4.j,]$concat,
                                    cc_concat2=cc[s3.j,]$concat2,
                                    adist=min.concat2[j]),match.s3.s4)
  }
    
cc_spots <- match.s3.s4 %>%
  left_join(dplyr::select(cc, last4ccnum, ID), by = c("cc.j" = "ID")) %>% #Add in CC num column
  left_join(dplyr::select(spots, RoleNName, No), by = c("spots.j" = "No")) #Add in Spots num column

```



```{r extract-cc-spots-pairs}

cc_spots_unique <- dcast(cc_spots, last4ccnum + RoleNName ~ adist) #Long to wide by transposing adist
cc_spots_unique$Total <- rowSums(cc_spots_unique[,c("0","1","2","3","4","5","11")]) #Sum all rows
cc_spots_unique$Sum01 <- rowSums(cc_spots_unique[,c("0","1")]) #Sum only column 1 and 2
cc_spots_unique$MatchPctTotal <- percent(cc_spots_unique[,3] / cc_spots_unique$Total) #Calc % of perfect (0) matches against Total
cc_spots_unique$MatchPct01 <- percent(cc_spots_unique$Sum01 / cc_spots_unique$Total) #Calc % of perfect (0) and almost perfect (0) matches against Total

cc_loyalty_unique_80 <- cc_loyalty_unique %>% 
  filter (MatchPct01 >= "80.00%")
n_distinct(cc_loyalty_unique_80$last4ccnum)
n_distinct(cc_loyalty_unique_80$loyaltynum)

write_csv(cc_loyalty_unique_80,"C:\\Users\\syeda\\OneDrive\\Documents\\SMU Courses\\2021T3 ISSS608 Visual Analytics and Applications\\3. Submissions\\Assignments\\Sandbox\\cc_loyalty_unique_80.csv")

```





```{r GPS-lines}

gps_sf <- st_as_sf(gps, coords = c("long", "lat"), # Changing into a shapefile
    crs = 4326, agr = "constant")
spots_sf <- st_as_sf(spots, coords = c("long", "lat"), # Changing into a shapefile
    crs = 4326, agr = "constant")
gps_path <- gps_sf %>% # Creating a movement path
  group_by(id) %>%
  summarize(m = mean(Timestamp), 
            do_union=FALSE) %>%
  st_cast("LINESTRING")


#spots_l <- spots[,4:5] # Extract only lat long
#spots_ll <- spots_l %>% slice(rep(1:n(), each = 3648))
#spots_ll$No <- rep(1:3648, times = 3648)
#spots_l$Noo <- rep(1:3648, times = 1)
#spots_ll <- left_join(spots_ll,spots_l, by = c("No" = "Noo"))
#spots_ll$Diff <- sqrt((spots_ll$lat.x-spots_ll$lat.y)^2+(spots_ll$long.x-spots_ll$long.y)^2)

#n_perc(spots_ll$Diff < 0.05)

#write_csv(spots,"C:\\Users\\syeda\\OneDrive\\Desktop\\spots.csv")

# Mapping map and gps together specifically for CarID #1
#mapping_spots <- ggplot(spots, aes(long, lat)) +
#  annotation_custom(rasterGrob(mc2,
#    width = unit(1, "npc"),
#    height = unit(1,"npc")),
#  xmin = 24.8244, xmax = 24.9096, ymin = 36.0453, ymax = 36.0952) + 
#  geom_point(size = 0.1) + 
#  coord_fixed(xlim = c(24.8244, 24.9096), ylim = c(36.0453, 36.0952)) + # Fixing the scales regardless of filtering of points
#  theme_bw() + theme(panel.border = element_blank(), # Remove background and grids and reformat scales and axis
#                     panel.grid.major = element_blank(), 
#                     panel.grid.minor = element_blank(), 
#                     axis.line = element_line(colour = "black"))  

#mapping_spots

```



Through a visual inspection of the map, credit card and loyalty data, we found 41 unique locations. With this, we then used the spots data above to find the coordinates nearest to these locations. We recorded the coordinates of each location in an Excel file, and would now bring it into R.

```{r spot-locations}

spot_list <- read_excel("data/spots and list.xlsx", sheet = "LIST") #Import Excel file

  


```



---

## 5.	Data Exploration Analysis

**Questions**

1 –– Using just the credit and loyalty card data, identify the most popular locations, and when they are popular. What anomalies do you see? What corrections would you recommend to correct these anomalies? Please limit your answer to 8 images and 300 words.

Provide your answer and corresponding images here.

1a. Using purely the credit card data, let's examine the locations by date using a calendar heatmap.

```{r}

cc_calendar <- cc %>% count(date, location)
cc_calendar_ggplot <- ggplot(complete(cc_calendar, date, location), aes(x = date, y = location)) + 
  geom_tile(aes(fill = n), color = "white", size = 0.1) +
  scale_fill_gradient(low = "light grey", high = "black", na.value = "light grey") +
  scale_x_date(date_labels = "%a \n %d %b", 
               date_breaks = "1 day") +
  scale_y_discrete(limits=rev) +
  labs(title = "Calendar Heatmap of Location Visit Frequency By Date",
       fill = "Frequency \n Of Visit") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())

cc_calendar_ggplot

loc1 <- "Katerina's Cafe"
cc_calendar_one <- cc %>%
  filter(location == loc1) %>%
  count(date,hour)
  
cc_calendar_ggplot_one <-  ggplot(complete(cc_calendar_one, date, hour), 
    aes(x = date, y = hour)) + 
  geom_tile(aes(fill = n), color = "white", size = 0.1) +
  scale_fill_gradient(low = "light grey", high = "black", na.value = "light grey") +
  scale_x_date(date_labels = "%a \n %d %b", 
               date_breaks = "1 day") +
  scale_y_reverse() +
  labs(title = paste(loc1,"Visit Frequency By Date And Hour"),
       fill = "Frequency \n Of Visit") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())

cc_calendar_ggplot_one

```


2 – Add the vehicle data to your analysis of the credit and loyalty card data. How does your assessment of the anomalies in question 1 change based on this new data? What discrepancies between vehicle, credit, and loyalty card data do you find? Please limit your answer to 8 images and 500 words.

Provide your answer and corresponding images here.

3 – Can you infer the owners of each credit card and loyalty card? What is your evidence? Where are there uncertainties in your method? Where are there uncertainties in the data? Please limit your answer to 8 images and 500 words.

Provide your answer and corresponding images here.

4 –– Given the data sources provided, identify potential informal or unofficial relationships among GASTech personnel. Provide evidence for these relationships. Please limit your response to 8 images and 500 words.

Provide your answer and corresponding images here.

5 –– Do you see evidence of suspicious activity? Identify 1- 10 locations where you believe the suspicious activity is occurring, and why    Please limit your response to 10 images and 500 words.

Provide your answer and corresponding images here.

6 –– If you solved this mini-challenge in 2014, how did you approach it differently this year?

Provide your answer here.






Testing the use of a static image as background and showcasing the gps data on it using ggplot

```{r}

# Mapping map and gps together specifically for CarID #1
#mapping <- ggplot(gps_name %>%
#                    filter(id == "1"),
#                  aes(long, lat)) +
#  annotation_custom(rasterGrob(mc2,
#    width = unit(1, "npc"),
#    height = unit(1,"npc")),
#  xmin = 24.8244, xmax = 24.9096, ymin = 36.0453, ymax = 36.0952) + #Original searching
#  xmin = 24.82419, xmax = 24.90976, ymin = 36.04499, ymax = 36.09543) + #Prof's raster extraction
#  geom_point(size = 0.1) + 
#  coord_fixed(xlim = c(24.8244, 24.9096), ylim = c(36.0453, 36.0952)) + # Fixing the scales regardless of filtering of points
#  theme_bw() + theme(panel.border = element_blank(), # Remove background and grids and reformat scales and axis
#                     panel.grid.major = element_blank(), 
#                     panel.grid.minor = element_blank(), 
#                     axis.line = element_line(colour = "black"))  
#  + transition_time(Timestamp) +
#  labs(title = "Date:{frame_time}")

#mapping

```

Testing the use of tmap

```{r tmap-visualisation}

gps_path_selected <- gps_path %>%
  filter(id==3)
tmap_mode("view")
tm_shape(mc2) +
  tm_rgb(mc2, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255) +
  tm_shape(gps_path_selected) +
  tm_lines()

```



```{r include=FALSE}
m <- tm_shape(mc2) +
  tm_rgb(mc2, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255) +
  tm_shape(gps_path) +
  tm_lines() +
  tm_facets(along = "id")
tmap_animation(m, 
               filename = "gif/drivers.gif",
               delay=100)
```







---
title: "VAST Challenge 2021: Mini-Challenge 2"
description: |
  A short description of the post.
author:
  - name: Syed Ahmad Zaki
    url: https://www.google.com/
date: 06-08-2021
output:
  distill::distill_article:
    self_contained: false
    code_folding: true
    toc: true
    toc_depth: 3
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Installing and loading necessary packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(ggplot2, 
               tidyverse, 
               readr, 
#               jpeg, 
               grid, 
               plotly, 
#               gganimate, 
               writexl, 
#               qwraps2, 
               reshape2, 
               scales, 
               sf, 
               raster, 
               tmap, 
               clock,
forcats)

```

**Team Member:**  
Syed Ahmad Zaki, Singapore Management University of Singapore, ahmadzaki.2020@mitb.smu.edu.sg   
Student Team:  YES  

**Tools Used:**  
Rmarkdown  

**Approximately how many hours were spent working on this submission in total?**  
Provide an estimate of the total number of hours worked on this submission by your entire team.  

**May we post your submission in the Visual Analytics Benchmark Repository after VAST Challenge 2021 is complete?**  
YES  

**Video**  
Provide a link to your video.  Example:
http://www.westbirmingham.ac.uk/uwb-smith-mc2-video.wmv

---

## Our Mission (Should We Accept It)
As a visual analytics expert assisting law enforcement, your mission is to identify which GASTech employees made which purchases and identify suspicious patterns of behavior.. You must cope with uncertainties that result from missing, conflicting, and imperfect data to make recommendations for further investigation.

Use visual analytics to analyze the available data and develop responses to the questions below. In addition, prepare a video that shows how you used visual analytics to solve this challenge. Submission instructions are available here. Entry forms are available for download below.
https://vast-challenge.github.io/2021/MC2.html

---

## 1.	Data Understanding
We start by loading all the necessary datasets provided in VAST Challenge 2021 Mini-Challenge 2.

```{r load-all-datasets, echo = T, results = 'hide'}

# Loading all datasets and image
cc <- readr::read_csv("data/cc_data.csv") # Add credit card data
loyalty <- readr::read_csv("data/loyalty_data.csv") # Add loyalty data
# mc2 <- jpeg::readJPEG("img/MC2-tourist.jpg", native = TRUE) # Add jpeg map file
mc2 <- raster("data/MC2-tourist_modified.tif") # Add tif file as a raster layer
gps <- readr::read_csv("data/gps.csv") # Add gps data
car <- readr::read_csv("data/car-assignments.csv") # Add car assignments
Abila_st <- st_read(dsn = "data", layer = "Abila")

```

### 1.1	Understanding the data
As always, we will review each of the datasets in greater detail. This is a necessary step in order to accurately prepare the data for subsequent use.

We will begin by reviewing both the csv data first We noticed a few discrepancies:  
1. Date format within the timestamp were in a MM-DD-YYYY H:M format  
2. Katerina's Cafe contains unique characters, which may cause problems during our analysis  
3. ID and Last4CCNum are treated as regular double numbers, instead of a character type.

```{r csv-data-cleaning}

str(cc)
str(loyalty)
str(car)
str(gps)

#--------------- Cleaning CC data ---------------

#cc$timestamp <- as.POSIXct(cc$timestamp, format = "%m/%d/%Y  %H:%M", tz = "GMT") #Readjust CC timestamp
cc$timestamp <- date_time_parse(cc$timestamp,
                zone = "",
                format = "%m/%d/%Y %H:%M") #Readjust CC timestamp
cc[grep("Katerina", cc$location),2] <- "Katerina's Cafe" #Replace unique characters in Katerina's Cafe
cc$last4ccnum <- as_factor(cc$last4ccnum)

#--------------- Cleaning Loyalty data ---------------

#loyalty$timestamp <- as.POSIXct(loyalty$timestamp, format = "%m/%d/%Y", tz = "GMT") #Readjust Loyalty timestamp
loyalty$timestamp <- date_time_parse(loyalty$timestamp,
                zone = "",
                format = "%m/%d/%Y") #Readjust CC timestamp
loyalty[grep("Katerina", loyalty$location),2] <- "Katerina's Cafe" #Replace unique characters in Katerina's Cafe

#--------------- Cleaning Car Assignment data ---------------

car$CarID <- as_factor(car$CarID)

#--------------- Cleaning GPS data ---------------

gps$id <- as_factor(gps$id)
gps$Timestamp <- date_time_parse(gps$Timestamp,
                zone = "",
                format = "%m/%d/%Y %H:%M:%S")

```


## 2.	Data Preparation
### 2.1	Combining Both Credit Card and Loyalty Data
We will now attempt to find matching rows between the cc and loyalty data. We will use [fuzzy string matching](https://www.r-bloggers.com/2015/02/fuzzy-string-matching-a-survival-skill-to-tackle-unstructured-information/) using [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) which is available natively in R's adist utilities package.

```{r levenshtein-distance-matching}

cc <- tibble::rowid_to_column(cc, "ID") #Create a numeric id column
loyalty <- tibble::rowid_to_column(loyalty, "ID") #Create a numeric id column
cc$date <- as.Date(cc$timestamp) #Create a separate column just for dates in the cc data
cc$hour <- as.numeric(format(cc$timestamp,"%H")) #Create a separate column just for hours in the cc data
cc$concat <- paste(cc$date,cc$location,cc$price) #Create a separate column of unique values using concatenated values in the cc data
loyalty$concat <- paste(loyalty$timestamp,loyalty$location,loyalty$price) #Create a separate column of unique values using concatenated values in the loyalty data
dist.concat <- adist(cc$concat,loyalty$concat, partial = TRUE, ignore.case = TRUE) #Creates a matrix with the Standard Levenshtein distance between both newly created concat columns
min.concat <- apply(dist.concat, 1, min) #Extract pairs with minimum distance

match.s1.s2 <- NULL
for (i in 1:nrow(dist.concat))
{
  s2.i <- match(min.concat[i], dist.concat[i,])
  s1.i <- i
  match.s1.s2 <- rbind(data.frame(loyalty.i=s2.i,
                                    cc.i=s1.i,
                                    loyalty_concat=loyalty[s2.i,]$concat,
                                    cc_concat=cc[s1.i,]$concat,
                                    adist=min.concat[i]),match.s1.s2)
  }
    
cc_loyalty <- match.s1.s2 %>%
  left_join(dplyr::select(cc, last4ccnum, ID), by = c("cc.i" = "ID")) %>% #Add in CC num column
  left_join(dplyr::select(loyalty, loyaltynum, ID), by = c("loyalty.i" = "ID")) #Add in loyalty card num column

```

Let's now extract the matching credit card-loyalty pairs according to 80% matching of their comparative distance.

```{r extract-pairs}

cc_loyalty_unique <- dcast(cc_loyalty, last4ccnum + loyaltynum ~ adist) #Long to wide by transposing adist
cc_loyalty_unique$Total <- rowSums(cc_loyalty_unique[,c("0","1","2","3","4","5","11")]) #Sum all rows
cc_loyalty_unique$Sum01 <- rowSums(cc_loyalty_unique[,c("0","1")]) #Sum only column 1 and 2
cc_loyalty_unique$MatchPctTotal <- percent(cc_loyalty_unique[,3] / cc_loyalty_unique$Total) #Calc % of perfect (0) matches against Total
cc_loyalty_unique$MatchPct01 <- percent(cc_loyalty_unique$Sum01 / cc_loyalty_unique$Total) #Calc % of perfect (0) and almost perfect (0) matches against Total

cc_loyalty_unique_80 <- cc_loyalty_unique %>% 
  filter (MatchPct01 >= "80.00%")
n_distinct(cc_loyalty_unique_80$last4ccnum)
n_distinct(cc_loyalty_unique_80$loyaltynum)

```









We will now merge the GPS data, together with the car assignments.

```{r GPS-data-manipulation}

# GPS Data Manipulation
gps_name <- left_join(gps,car, by = c("id" = "CarID")) # Merge car assignments to gps data
gps_name$Timestamp <- as.POSIXct(gps_name$Timestamp, format = "%m/%d/%Y  %H:%M:%S", tz = "GMT") # Timestamp switching to month-day-year format
gps_name <- gps_name[with(gps_name,order(id,Timestamp)),] # Sort first by ID in ascending order and then Timestamp by oldest to newest
gps_name <- gps_name %>% # Add running number in the first column
  mutate(No = 1:n()) %>% 
  dplyr::select(No, everything()) 
gps_name <- gps_name %>% # Create additional column indicating time taken from previous timestamp for same ID
    mutate(Delta = Timestamp - lag(Timestamp, default = first(Timestamp)))

gps_name$Delta <- as.numeric(gps_name$Delta)

spots <- gps_name %>% # Filtering out gps coordinates where stationary for more than 10 mins
  filter(Delta > 600)
spots$No <- rep(1:2965, times = 1) # Add running number in the first column

gps_sf <- st_as_sf(gps, coords = c("long", "lat"), # Changing into a shapefile
    crs = 4326, agr = "constant")
spots_sf <- st_as_sf(spots, coords = c("long", "lat"), # Changing into a shapefile
    crs = 4326, agr = "constant")
gps_path <- gps_sf %>% # Creating a movement path
  group_by(id) %>%
  summarize(m = mean(Timestamp), 
            do_union=FALSE) %>%
  st_cast("LINESTRING")


#spots_l <- spots[,4:5] # Extract only lat long
#spots_ll <- spots_l %>% slice(rep(1:n(), each = 3648))
#spots_ll$No <- rep(1:3648, times = 3648)
#spots_l$Noo <- rep(1:3648, times = 1)
#spots_ll <- left_join(spots_ll,spots_l, by = c("No" = "Noo"))
#spots_ll$Diff <- sqrt((spots_ll$lat.x-spots_ll$lat.y)^2+(spots_ll$long.x-spots_ll$long.y)^2)

#n_perc(spots_ll$Diff < 0.05)

#write_csv(spots,"C:\\Users\\syeda\\OneDrive\\Desktop\\spots.csv")

# Mapping map and gps together specifically for CarID #1
#mapping_spots <- ggplot(spots, aes(long, lat)) +
#  annotation_custom(rasterGrob(mc2,
#    width = unit(1, "npc"),
#    height = unit(1,"npc")),
#  xmin = 24.8244, xmax = 24.9096, ymin = 36.0453, ymax = 36.0952) + 
#  geom_point(size = 0.1) + 
#  coord_fixed(xlim = c(24.8244, 24.9096), ylim = c(36.0453, 36.0952)) + # Fixing the scales regardless of filtering of points
#  theme_bw() + theme(panel.border = element_blank(), # Remove background and grids and reformat scales and axis
#                     panel.grid.major = element_blank(), 
#                     panel.grid.minor = element_blank(), 
#                     axis.line = element_line(colour = "black"))  

#mapping_spots

```



## 3.	Data Exploration Analysis

---

**Questions**

1 –– Using just the credit and loyalty card data, identify the most popular locations, and when they are popular. What anomalies do you see? What corrections would you recommend to correct these anomalies? Please limit your answer to 8 images and 300 words.

Provide your answer and corresponding images here.

1a. Using purely the credit card data, let's examine the locations by date using a calendar heatmap.

```{r}

cc_calendar <- cc %>% count(date, location)
cc_calendar_ggplot <- ggplot(complete(cc_calendar, date, location), aes(x = date, y = location)) + 
  geom_tile(aes(fill = n), color = "white", size = 0.1) +
  scale_fill_gradient(low = "light grey", high = "black", na.value = "light grey") +
  scale_x_date(date_labels = "%a \n %d %b", 
               date_breaks = "1 day") +
  scale_y_discrete(limits=rev) +
  labs(title = "Calendar Heatmap of Location Visit Frequency By Date",
       fill = "Frequency \n Of Visit") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())

cc_calendar_ggplot

loc1 <- "Katerina's Cafe"
cc_calendar_one <- cc %>%
  filter(location == loc1) %>%
  count(date,hour)
  
cc_calendar_ggplot_one <-  ggplot(complete(cc_calendar_one, date, hour), 
    aes(x = date, y = hour)) + 
  geom_tile(aes(fill = n), color = "white", size = 0.1) +
  scale_fill_gradient(low = "light grey", high = "black", na.value = "light grey") +
  scale_x_date(date_labels = "%a \n %d %b", 
               date_breaks = "1 day") +
  scale_y_reverse() +
  labs(title = paste(loc1,"Visit Frequency By Date And Hour"),
       fill = "Frequency \n Of Visit") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank())

cc_calendar_ggplot_one

```


2 – Add the vehicle data to your analysis of the credit and loyalty card data. How does your assessment of the anomalies in question 1 change based on this new data? What discrepancies between vehicle, credit, and loyalty card data do you find? Please limit your answer to 8 images and 500 words.

Provide your answer and corresponding images here.

3 – Can you infer the owners of each credit card and loyalty card? What is your evidence? Where are there uncertainties in your method? Where are there uncertainties in the data? Please limit your answer to 8 images and 500 words.

Provide your answer and corresponding images here.

4 –– Given the data sources provided, identify potential informal or unofficial relationships among GASTech personnel. Provide evidence for these relationships. Please limit your response to 8 images and 500 words.

Provide your answer and corresponding images here.

5 –– Do you see evidence of suspicious activity? Identify 1- 10 locations where you believe the suspicious activity is occurring, and why    Please limit your response to 10 images and 500 words.

Provide your answer and corresponding images here.

6 –– If you solved this mini-challenge in 2014, how did you approach it differently this year?

Provide your answer here.






Testing the use of a static image as background and showcasing the gps data on it using ggplot

```{r}



# Mapping map and gps together specifically for CarID #1
mapping <- ggplot(gps_name %>%
                    filter(id == "1"),
                  aes(long, lat)) +
  annotation_custom(rasterGrob(mc2,
    width = unit(1, "npc"),
    height = unit(1,"npc")),
#  xmin = 24.8244, xmax = 24.9096, ymin = 36.0453, ymax = 36.0952) + #Original searching
  xmin = 24.82419, xmax = 24.90976, ymin = 36.04499, ymax = 36.09543) + #Prof's raster extraction
  geom_point(size = 0.1) + 
  coord_fixed(xlim = c(24.8244, 24.9096), ylim = c(36.0453, 36.0952)) + # Fixing the scales regardless of filtering of points
  theme_bw() + theme(panel.border = element_blank(), # Remove background and grids and reformat scales and axis
                     panel.grid.major = element_blank(), 
                     panel.grid.minor = element_blank(), 
                     axis.line = element_line(colour = "black"))  
#  + transition_time(Timestamp) +
#  labs(title = "Date:{frame_time}")

mapping

```

Testing the use of tmap

```{r tmap-visualisation}

gps_path_selected <- gps_path %>%
  filter(id==3)
tmap_mode("view")
tm_shape(mc2) +
  tm_rgb(mc2, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255) +
  tm_shape(gps_path_selected) +
  tm_lines()

```



```{r}
m <- tm_shape(mc2) +
  tm_rgb(mc2, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255) +
  tm_shape(gps_path) +
  tm_lines() +
  tm_facets(along = "id")
tmap_animation(m, 
               filename = "gif/drivers.gif",
               delay=100)
```






